{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Biw000/Biw000/blob/main/YOLO%20V8n-YOLO%20V11n\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOLO V8n-YOLO V11n"
      ],
      "metadata": {
        "id": "PApm7yt785iP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# yolo_train_minimal.py (FAST GPU, compile-safe, auto Confusion Matrix)\n",
        "# Bootstrap dataset -> Preprocess (jpg/resize) -> Clean outliers -> Train YOLO v8/11 -> Export PNG/CSV\n",
        "import os, sys, shutil, random, time, warnings, re, yaml, traceback, math, csv\n",
        "from pathlib import Path\n",
        "from statistics import mean, stdev\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "CONFIG = {\n",
        "    # ---- I/O ----\n",
        "    \"SOURCE_DIR\":   r\"E:\\dataset\\preview\",\n",
        "    \"DATASET_ROOT\": r\"E:\\YOLO\\dataset\",\n",
        "    \"PROJECT_DIR\":  r\"E:\\YOLO\\RUNS\",\n",
        "    \"OUTPUTS_DIR\":  r\"E:\\YOLO\\END\",\n",
        "\n",
        "    # ---- Classes (‡∏ñ‡πâ‡∏≤ SOURCE_DIR ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏¢‡πà‡∏≠‡∏¢ ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ä‡∏∑‡πà‡∏≠ object) ----\n",
        "    \"CLASSES\": [\"\"],\n",
        "\n",
        "    # ---- Split ----\n",
        "    \"SPLIT\": (0.70, 0.15, 0.15),\n",
        "\n",
        "    # ---- Preprocess ----\n",
        "    \"PRE_RESIZE\": 1024,\n",
        "    \"SQUARE\": False,\n",
        "    \"JPEG_QUALITY\": 90,\n",
        "\n",
        "    # ---- Training (tuned for speed on GPU) ----\n",
        "    \"VARIANTS\": [\"yolov8n\", \"yolo11n\"],\n",
        "    \"EPOCHS\": 100,\n",
        "    \"IMGSZ\": 448,\n",
        "    \"BATCH\": -1,                # auto ‡πÇ‡∏î‡∏¢ Ultralytics\n",
        "    \"DEVICE\": \"GPU\",           # ‡πÉ‡∏ä‡πâ GPU ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏ñ‡πâ‡∏≤‡∏°‡∏µ\n",
        "    \"WORKERS\": \"auto\",          # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° CPU core\n",
        "    \"PATIENCE\": 25,\n",
        "    \"CLOSE_MOSAIC\": 10,\n",
        "    \"CACHE\": \"ram\",             # ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏∏‡∏î\n",
        "    \"AMP\": True,\n",
        "    \"OPTIMIZER\": \"auto\",\n",
        "    \"COS_LR\": True,\n",
        "    \"COMPILE\": True,            # ‡∏•‡∏≠‡∏á torch.compile ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ\n",
        "\n",
        "    # ---- Cleaning ----\n",
        "    \"MIN_SIDE\": 64,\n",
        "    \"ASPECT_Z\": 3.0,\n",
        "    \"SIZE_Z\": 3.0,\n",
        "\n",
        "    \"SEED\": 42,\n",
        "}\n",
        "\n",
        "IMG_EXTS = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\"}\n",
        "\n",
        "def _ensure_pkgs():\n",
        "    try:\n",
        "        import torch; from ultralytics import YOLO  # noqa: F401\n",
        "        import cv2, numpy as np                      # noqa: F401\n",
        "        import PIL, matplotlib                       # noqa: F401\n",
        "    except Exception:\n",
        "        print(\"‚ùå ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏Å‡πà‡∏≠‡∏ô:\\n  pip install -U ultralytics torch torchvision torchaudio pillow matplotlib opencv-python pyyaml tqdm numpy\")\n",
        "        sys.exit(0)\n",
        "\n",
        "def set_seed_and_fast(seed=42):\n",
        "    # ‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏£‡πá‡∏ß: ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ cuDNN ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å kernel ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
        "    import torch, numpy as np\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    try:\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.benchmark = True      # auto-tune\n",
        "        torch.backends.cudnn.deterministic = False # ‡πÑ‡∏°‡πà deterministic ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß\n",
        "        try:\n",
        "            torch.set_float32_matmul_precision('high')\n",
        "        except Exception:\n",
        "            pass\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def pick_device(device_cfg=\"auto\"):\n",
        "    import torch\n",
        "    if device_cfg == \"auto\":\n",
        "        return \"0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    return device_cfg\n",
        "\n",
        "def auto_workers(workers_cfg=\"auto\"):\n",
        "    if workers_cfg != \"auto\":\n",
        "        return workers_cfg\n",
        "    try:\n",
        "        import multiprocessing as mp\n",
        "        cpu = mp.cpu_count()\n",
        "        return max(4, min(12, (cpu - 2)))  # heuristic\n",
        "    except Exception:\n",
        "        return 4\n",
        "\n",
        "def gpu_info():\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.cuda.is_available():\n",
        "            return f\"GPU: {torch.cuda.get_device_name(0)}\"\n",
        "        return \"GPU: None (CPU)\"\n",
        "    except Exception:\n",
        "        return \"GPU: Unknown\"\n",
        "\n",
        "def create_yolo_dirs(root: Path):\n",
        "    for sp in [\"train\",\"val\",\"test\"]:\n",
        "        (root/\"images\"/sp).mkdir(parents=True, exist_ok=True)\n",
        "        (root/\"labels\"/sp).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def scan_source(source: Path):\n",
        "    subdirs = [p for p in source.iterdir() if p.is_dir()] if source.exists() else []\n",
        "    if subdirs:\n",
        "        out={}\n",
        "        for d in sorted(subdirs):\n",
        "            imgs=[p for p in d.rglob(\"*\") if p.suffix.lower() in IMG_EXTS]\n",
        "            if imgs: out[d.name]=imgs\n",
        "        return out\n",
        "    # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏¢‡πà‡∏≠‡∏¢ -> ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "    fallback = (CONFIG[\"CLASSES\"][0].strip() if CONFIG[\"CLASSES\"] else \"\") or \"object\"\n",
        "    imgs=[p for p in source.rglob(\"*\") if p.suffix.lower() in IMG_EXTS]\n",
        "    return {fallback: imgs}\n",
        "\n",
        "def split_list(lst, ratios):\n",
        "    lst=lst[:]; random.shuffle(lst)\n",
        "    n=len(lst); a=int(n*ratios[0]); b=int(n*(ratios[0]+ratios[1]))\n",
        "    return lst[:a], lst[a:b], lst[b:]\n",
        "\n",
        "def safe_open_image(path: Path):\n",
        "    from PIL import Image\n",
        "    im=Image.open(path); im.load(); return im\n",
        "\n",
        "def ensure_jpg_resize(src: Path, dst_noext: Path, pre_resize: int, square: bool, jpeg_quality: int):\n",
        "    from PIL import Image\n",
        "    im=safe_open_image(src).convert(\"RGB\")\n",
        "    if pre_resize and pre_resize>0:\n",
        "        w,h=im.size; ms=max(w,h)\n",
        "        if ms>pre_resize:\n",
        "            s=pre_resize/float(ms)\n",
        "            im=im.resize((max(1,int(w*s)), max(1,int(h*s))), Image.BICUBIC)\n",
        "    if square:\n",
        "        w,h=im.size\n",
        "        if w!=h:\n",
        "            size=max(w,h)\n",
        "            bg=Image.new(\"RGB\",(size,size),(0,0,0))\n",
        "            bg.paste(im, ((size-w)//2, (size-h)//2))\n",
        "            im=bg\n",
        "    dst=dst_noext.with_suffix(\".jpg\")\n",
        "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "    im.save(dst, format=\"JPEG\", quality=jpeg_quality, optimize=True)\n",
        "    return dst\n",
        "\n",
        "def copy_with_label(src_img: Path, dst_img_noext: Path, dst_lbl: Path, allow_negative=True):\n",
        "    dst_lbl=dst_lbl.with_suffix(\".txt\")\n",
        "    src_lbl=src_img.with_suffix(\".txt\")\n",
        "    if src_lbl.exists():\n",
        "        shutil.copy2(src_lbl, dst_lbl)\n",
        "    else:\n",
        "        if allow_negative:\n",
        "            with open(dst_lbl,\"w\",encoding=\"utf-8\") as f: f.write(\"\")\n",
        "        else:\n",
        "            (dst_img_noext.with_suffix(\".jpg\")).unlink(missing_ok=True)\n",
        "\n",
        "def write_data_yaml(root: Path, class_names):\n",
        "    class_names=[c if str(c).strip() else \"object\" for c in class_names]\n",
        "    data={\"train\":str((root/\"images\"/\"train\").as_posix()),\n",
        "          \"val\":  str((root/\"images\"/\"val\").as_posix()),\n",
        "          \"test\": str((root/\"images\"/\"test\").as_posix()),\n",
        "          \"names\":class_names}\n",
        "    with open(root/\"data.yaml\",\"w\",encoding=\"utf-8\") as f:\n",
        "        yaml.safe_dump(data, f, allow_unicode=True, sort_keys=False)\n",
        "\n",
        "def safe_read_img_shape(img_path: Path):\n",
        "    try:\n",
        "        import cv2, numpy as np\n",
        "        im = cv2.imdecode(np.fromfile(str(img_path), dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
        "        if im is None: return None\n",
        "        h,w = im.shape[:2]; return (w,h)\n",
        "    except Exception:\n",
        "        try:\n",
        "            from PIL import Image\n",
        "            with Image.open(img_path) as im: return im.size\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "def collect_class_map(images_split_dir: Path):\n",
        "    labels_root = images_split_dir.parent.parent / \"labels\" / images_split_dir.name\n",
        "    class_to_images = {}\n",
        "    imgs = [p for p in images_split_dir.rglob(\"*\") if p.suffix.lower() in IMG_EXTS or p.suffix.lower()==\".jpg\"]\n",
        "    for img in imgs:\n",
        "        rel = img.relative_to(images_split_dir).with_suffix(\".txt\")\n",
        "        lbl = labels_root / rel\n",
        "        if not lbl.exists():\n",
        "            cids = [\"__nolabel__\"]\n",
        "        else:\n",
        "            try:\n",
        "                lines = [ln.strip() for ln in lbl.read_text(encoding=\"utf-8\").splitlines() if ln.strip()]\n",
        "                cids = []\n",
        "                for ln in lines:\n",
        "                    parts = re.split(r\"\\s+\", ln)\n",
        "                    if len(parts)>=5 and parts[0].isdigit():\n",
        "                        cids.append(parts[0])\n",
        "                if not cids: cids=[\"__nolabel__\"]\n",
        "            except Exception:\n",
        "                cids=[\"__nolabel__\"]\n",
        "        for cid in set(cids):\n",
        "            class_to_images.setdefault(cid, []).append(img)\n",
        "    return class_to_images\n",
        "\n",
        "def remove_outliers(images_dir: Path, min_side=64, aspect_z=3.0, size_z=3.0, log_file: Path=None):\n",
        "    \"\"\"‡πÑ‡∏°‡πà‡∏•‡∏ö __nolabel__; ‡∏•‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ/‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô/‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏∏‡∏î‡πÇ‡∏ï‡πà‡∏á\"\"\"\n",
        "    cmap = collect_class_map(images_dir)\n",
        "    removed=0; logs=[]\n",
        "    for cid, imgs in cmap.items():\n",
        "        if cid==\"__nolabel__\":\n",
        "            for p in imgs: logs.append(f\"{cid},kept_negative,{p}\")\n",
        "            continue\n",
        "        sizes=[]; aspects=[]; valids=[]\n",
        "        for p in imgs:\n",
        "            wh=safe_read_img_shape(p)\n",
        "            if not wh:\n",
        "                sizes.append(None); aspects.append(None); valids.append(False); continue\n",
        "            w,h=wh; ok=(w>=min_side and h>=min_side and h>0)\n",
        "            sizes.append((w*h) if ok else None)\n",
        "            aspects.append((w/h) if ok else None)\n",
        "            valids.append(ok)\n",
        "        def zscores(arr):\n",
        "            if len(arr)<3: return [0.0]*len(arr)\n",
        "            m=mean(arr); s=stdev(arr)\n",
        "            if s==0: return [0.0]*len(arr)\n",
        "            return [(x-m)/s for x in arr]\n",
        "        if any(s is not None for s in sizes):\n",
        "            _zs = zscores([s for s in sizes if s is not None]); it=iter(_zs)\n",
        "            sz_z=[next(it) if s is not None else float(\"inf\") for s in sizes]\n",
        "        else: sz_z=[0.0]*len(sizes)\n",
        "        if any(a is not None for a in aspects):\n",
        "            _za = zscores([a for a in aspects if a is not None]); it=iter(_za)\n",
        "            ar_z=[next(it) if a is not None else float(\"inf\") for a in aspects]\n",
        "        else: ar_z=[0.0]*len(aspects)\n",
        "        for i,p in enumerate(imgs):\n",
        "            bad=False; reason=\"\"\n",
        "            if not valids[i]: bad=True; reason=\"invalid_dim_or_read\"\n",
        "            elif abs(sz_z[i])>size_z: bad=True; reason=f\"size_z|{sz_z[i]:.2f}\"\n",
        "            elif abs(ar_z[i])>aspect_z: bad=True; reason=f\"aspect_z|{ar_z[i]:.2f}\"\n",
        "            if bad:\n",
        "                try:\n",
        "                    p.unlink()\n",
        "                    lbl=(images_dir.parent.parent/\"labels\"/images_dir.name/p.relative_to(images_dir)).with_suffix(\".txt\")\n",
        "                    lbl.unlink(missing_ok=True)\n",
        "                    logs.append(f\"{cid},{reason},{p}\"); removed+=1\n",
        "                except Exception:\n",
        "                    pass\n",
        "    if log_file:\n",
        "        log_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(log_file,\"w\",encoding=\"utf-8\") as f:\n",
        "            f.write(\"class_id,reason,path\\n\")\n",
        "            for ln in logs: f.write(ln+\"\\n\")\n",
        "    return removed\n",
        "\n",
        "# ---- ‡∏ï‡∏£‡∏ß‡∏à‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á (‡∏•‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏°‡∏µ/‡πÑ‡∏°‡πà‡∏°‡∏µ .pt) ----\n",
        "def resolve_variant_name(name: str):\n",
        "    from ultralytics import YOLO\n",
        "    tries=[name]\n",
        "    if name.endswith(\".pt\"): tries.append(name[:-3])\n",
        "    else: tries.append(name + \".pt\")\n",
        "    for cand in tries:\n",
        "        try:\n",
        "            YOLO(cand)  # test load\n",
        "            return cand\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "# ---- ‡πÄ‡∏ó‡∏£‡∏ô: ‡πÄ‡∏£‡πá‡∏ß/‡∏Å‡∏¥‡∏ô‡πÅ‡∏£‡∏°‡∏ô‡πâ‡∏≠‡∏¢ + ‡πÅ‡∏™‡∏î‡∏á traceback + fallback ----\n",
        "def train_one(weights, data_yaml, project_dir, name, do_val=True):\n",
        "    from ultralytics import YOLO\n",
        "    run_name = f\"{name}_{int(time.time())}\"\n",
        "    model = YOLO(weights)\n",
        "\n",
        "    # ‡πÉ‡∏ä‡πâ torch.compile ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ PyTorch 2.x (‡∏ö‡∏≤‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏Ñ‡∏∑‡∏ô None)\n",
        "    if CONFIG.get(\"COMPILE\", False):\n",
        "        try:\n",
        "            compiled_obj = model.compile()\n",
        "            if compiled_obj is not None:\n",
        "                model = compiled_obj\n",
        "            if not hasattr(model, \"train\"):\n",
        "                print(\"‚ö†Ô∏è compile() ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå ‚Üí ‡∏õ‡∏¥‡∏î COMPILE ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ\")\n",
        "            else:\n",
        "                print(\"‚úÖ torch.compile enabled\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ÑπÔ∏è compile() ‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ö‡∏ô‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‚Äî ‡∏Ç‡πâ‡∏≤‡∏° ({e})\")\n",
        "\n",
        "    if not hasattr(model, \"train\"):\n",
        "        model = YOLO(weights)  # re-init clean\n",
        "        print(\"‚Ü∫ fallback: ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà compile\")\n",
        "\n",
        "    def _run(_batch, _workers, _cache, _val):\n",
        "        start = time.time()\n",
        "        results = model.train(\n",
        "            data=str(Path(data_yaml).as_posix()),\n",
        "            epochs=CONFIG[\"EPOCHS\"],\n",
        "            imgsz=CONFIG[\"IMGSZ\"],\n",
        "            batch=(CONFIG[\"BATCH\"] if _batch is None else _batch),\n",
        "            workers=(auto_workers(CONFIG[\"WORKERS\"]) if _workers is None else _workers),\n",
        "            device=pick_device(CONFIG[\"DEVICE\"]),\n",
        "            project=str(Path(project_dir).as_posix()),\n",
        "            name=run_name,\n",
        "            exist_ok=True,\n",
        "            amp=CONFIG[\"AMP\"],\n",
        "            optimizer=CONFIG[\"OPTIMIZER\"],\n",
        "            cos_lr=CONFIG[\"COS_LR\"],\n",
        "            cache=(CONFIG[\"CACHE\"] if _cache is None else _cache),\n",
        "            patience=CONFIG[\"PATIENCE\"],\n",
        "            close_mosaic=CONFIG[\"CLOSE_MOSAIC\"],\n",
        "            pretrained=True,\n",
        "            verbose=True,\n",
        "            val=_val,\n",
        "            plots=False,\n",
        "            rect=True,\n",
        "            save_json=False,\n",
        "            save_period=-1,\n",
        "        )\n",
        "        dur = time.time() - start\n",
        "        return Path(results.save_dir), dur\n",
        "\n",
        "    try:\n",
        "        return _run(None, None, None, do_val)\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è ‡πÄ‡∏ó‡∏£‡∏ô‡∏£‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏•‡πâ‡∏°:\", repr(e)); import traceback; print(traceback.format_exc())\n",
        "\n",
        "    # Fallback ‡πÇ‡∏´‡∏°‡∏î‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î (‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ GPU ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)\n",
        "    try:\n",
        "        print(\"‚Ü∫ ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏°‡∏î‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î: batch=8, workers=0, cache='disk', val=False\")\n",
        "        return _run(_batch=8, _workers=0, _cache='disk', _val=False)\n",
        "    except Exception as e2:\n",
        "        print(\"‚ùå ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡πÅ‡∏°‡πâ‡πÇ‡∏´‡∏°‡∏î‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î:\", repr(e2)); import traceback; print(traceback.format_exc())\n",
        "        return None, 0.0\n",
        "\n",
        "# ---- Val ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Confusion Matrix ‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏ó‡∏£‡∏ô ----\n",
        "def generate_confusion_matrix_png(weights_path, data_yaml, run_dir: Path, split=\"val\",\n",
        "                                  iou=0.50, conf=0.25):\n",
        "    \"\"\"\n",
        "    ‡∏™‡∏£‡πâ‡∏≤‡∏á Confusion Matrix ‡∏î‡πâ‡∏ß‡∏¢ Ultralytics ‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏ó‡∏£‡∏ô:\n",
        "    - weights_path: ‡∏û‡∏≤‡∏ò‡πÑ‡∏õ‡∏ó‡∏µ‡πà best.pt ‡∏Ç‡∏≠‡∏á‡∏£‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô\n",
        "    - data_yaml: data.yaml\n",
        "    - run_dir: ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏£‡∏±‡∏ô (‡∏à‡∏∞‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å cm ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà)\n",
        "    - split: 'val' ‡∏´‡∏£‡∏∑‡∏≠ 'test'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from ultralytics import YOLO\n",
        "        model = YOLO(weights_path)\n",
        "        # ‡∏£‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Ultralytics ‡∏™‡∏£‡πâ‡∏≤‡∏á confusion_matrix.png\n",
        "        results = model.val(\n",
        "            data=str(Path(data_yaml).as_posix()),\n",
        "            split=split,\n",
        "            plots=True,          # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏π‡∏õ\n",
        "            save_json=False,\n",
        "            iou=iou,\n",
        "            conf=conf,\n",
        "            imgsz=CONFIG[\"IMGSZ\"],\n",
        "            device=pick_device(CONFIG[\"DEVICE\"]),\n",
        "            workers=auto_workers(CONFIG[\"WORKERS\"]),\n",
        "            batch=CONFIG[\"BATCH\"] if isinstance(CONFIG[\"BATCH\"], int) and CONFIG[\"BATCH\"] > 0 else 16,\n",
        "            verbose=True\n",
        "        )\n",
        "        cm_src = Path(results.save_dir) / \"confusion_matrix.png\"\n",
        "        if cm_src.exists():\n",
        "            cm_dst = run_dir / \"confusion_matrix.png\"\n",
        "            shutil.copy2(cm_src, cm_dst)\n",
        "            print(f\"üß≠ Confusion Matrix saved -> {cm_dst}\")\n",
        "            return str(cm_dst)\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö confusion_matrix.png ‡∏à‡∏≤‡∏Å‡∏Ç‡∏±‡πâ‡∏ô val\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è ‡∏ó‡∏≥ Confusion Matrix ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}\")\n",
        "        import traceback; print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "def extract_metrics(run_dir: Path):\n",
        "    out={\"run_dir\":str(run_dir),\"variant\":run_dir.name}\n",
        "    res=run_dir/\"results.csv\"\n",
        "    if not res.exists(): return out\n",
        "    rows=[]\n",
        "    with open(res,\"r\",encoding=\"utf-8\") as f:\n",
        "        rd=csv.DictReader(f)\n",
        "        for r in rd: rows.append(r)\n",
        "    if not rows: return out\n",
        "\n",
        "    def tf(x):\n",
        "        try: return float(x)\n",
        "        except: return float(\"nan\")\n",
        "\n",
        "    def get_map(row):\n",
        "        for k in [\"metrics/mAP50-95(B)\",\"metrics/mAP50-95(M)\",\"metrics/mAP50-95\"]:\n",
        "            if k in row and row[k] not in (\"\",None): return tf(row[k])\n",
        "        return float(\"nan\")\n",
        "\n",
        "    best_i = max(range(len(rows)), key=lambda i: get_map(rows[i]))\n",
        "    best = rows[best_i]\n",
        "\n",
        "    def pick(keys, default=\"\"):\n",
        "        for k in keys:\n",
        "            if k in best: return best[k]\n",
        "        return default\n",
        "\n",
        "    out[\"best_epoch\"]=best.get(\"epoch\",best_i)\n",
        "    out[\"maps\"]=pick([\"metrics/mAP50-95(B)\",\"metrics/mAP50-95(M)\",\"metrics/mAP50-95\"],\"0\")\n",
        "    out[\"map50\"]=pick([\"metrics/mAP50(B)\",\"metrics/mAP50(M)\",\"metrics/mAP50\"],\"0\")\n",
        "    out[\"precision\"]=pick([\"metrics/precision(B)\",\"metrics/precision(M)\",\"metrics/precision\"],\"0\")\n",
        "    out[\"recall\"]=pick([\"metrics/recall(B)\",\"metrics/recall(M)\",\"metrics/recall\"],\"0\")\n",
        "    out[\"box_loss\"]=pick([\"loss/box(B)\",\"train/box_loss\",\"loss/box(M)\"],\"\")\n",
        "    out[\"cls_loss\"]=pick([\"loss/cls(B)\",\"train/cls_loss\",\"loss/cls(M)\"],\"\")\n",
        "    out[\"dfl_loss\"]=pick([\"loss/dfl(B)\",\"train/dfl_loss\",\"loss/dfL(M)\"],\"\") if False else pick([\"loss/dfl(B)\",\"train/dfl_loss\",\"loss/dfl(M)\"],\"\")\n",
        "    out[\"results_png\"]=str(run_dir/\"results.png\")\n",
        "    out[\"cm_png\"]=str(run_dir/\"confusion_matrix.png\")\n",
        "    out[\"weights\"]=str(run_dir/\"weights\"/\"best.pt\")\n",
        "    return out\n",
        "\n",
        "def plot_per_epoch(run_dir: Path, out_prefix: Path):\n",
        "    import matplotlib\n",
        "    matplotlib.use(\"Agg\")\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    xs=[]; mapv=[]; pv=[]; rv=[]; bl=[]; cl=[]; dl=[]\n",
        "    res=run_dir/\"results.csv\"\n",
        "    if not res.exists(): return\n",
        "    with open(res,\"r\",encoding=\"utf-8\") as f:\n",
        "        rd=csv.DictReader(f)\n",
        "        for r in rd:\n",
        "            def fget(k, alt=None):\n",
        "                v=r.get(k, r.get(alt,\"nan\") if alt else \"nan\")\n",
        "                try: return float(v)\n",
        "                except: return float(\"nan\")\n",
        "            xs.append(fget(\"epoch\",\"0\"))\n",
        "            mapv.append(fget(\"metrics/mAP50-95(B)\",\"metrics/mAP50-95(M)\"))\n",
        "            pv.append(fget(\"metrics/precision(B)\",\"metrics/precision(M)\"))\n",
        "            rv.append(fget(\"metrics/recall(B)\",\"metrics/recall(M)\"))\n",
        "            bl.append(fget(\"loss/box(B)\",\"train/box_loss\"))\n",
        "            cl.append(fget(\"loss/cls(B)\",\"train/cls_loss\"))\n",
        "            dl.append(fget(\"loss/dfl(B)\",\"train/dfl_loss\"))\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(xs,mapv,label=\"mAP50-95 (val)\")\n",
        "    plt.plot(xs,pv, label=\"Precision (val)\")\n",
        "    plt.plot(xs,rv, label=\"Recall (val)\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Score\"); plt.title(f\"Metrics - {run_dir.name}\")\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(out_prefix.with_name(out_prefix.stem+\"_metrics.png\")); plt.close()\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(xs,bl,label=\"Box loss\"); plt.plot(xs,cl,label=\"Cls loss\"); plt.plot(xs,dl,label=\"DFL loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"Losses - {run_dir.name}\")\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(out_prefix.with_name(out_prefix.stem+\"_losses.png\")); plt.close()\n",
        "\n",
        "def save_summary_csv(summaries, out_csv: Path):\n",
        "    keys=[\"variant\",\"best_epoch\",\"maps\",\"map50\",\"precision\",\"recall\",\"box_loss\",\"cls_loss\",\"dfl_loss\",\"weights\",\"run_dir\"]\n",
        "    with open(out_csv,\"w\",newline=\"\",encoding=\"utf-8\") as f:\n",
        "        w=csv.DictWriter(f,fieldnames=keys); w.writeheader()\n",
        "        for s in summaries: w.writerow({k:s.get(k,\"\") for k in keys})\n",
        "\n",
        "def compare_bar_png(summaries, out_png: Path):\n",
        "    import matplotlib\n",
        "    matplotlib.use(\"Agg\")\n",
        "    import matplotlib.pyplot as plt, numpy as np\n",
        "    labels=[s[\"variant\"] for s in summaries]\n",
        "    mAP=[float(s.get(\"maps\",0) or 0) for s in summaries]\n",
        "    P  =[float(s.get(\"precision\",0) or 0) for s in summaries]\n",
        "    R  =[float(s.get(\"recall\",0) or 0) for s in summaries]\n",
        "    x=np.arange(len(labels)); w=0.25\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.bar(x-w, mAP, w, label=\"mAP50-95\")\n",
        "    plt.bar(x,   P,   w, label=\"Precision\")\n",
        "    plt.bar(x+w, R,   w, label=\"Recall\")\n",
        "    plt.xticks(x, labels, rotation=10)\n",
        "    plt.ylabel(\"Score\"); plt.title(\"YOLO Variants Comparison\")\n",
        "    plt.legend(); plt.tight_layout(); plt.savefig(out_png); plt.close()\n",
        "\n",
        "def export_variant_artifacts(summary, outputs_dir: Path, run_dir: Path):\n",
        "    vdir = outputs_dir / summary[\"variant\"]\n",
        "    vdir.mkdir(parents=True, exist_ok=True)\n",
        "    for k in [\"results_png\", \"cm_png\", \"weights\"]:\n",
        "        src = Path(summary.get(k, \"\"))\n",
        "        if src.exists():\n",
        "            shutil.copy2(src, vdir / src.name)\n",
        "    for suf in [\"_metrics.png\", \"_losses.png\"]:\n",
        "        cand = outputs_dir / f\"per_epoch_{run_dir.name}{suf}\"\n",
        "        if cand.exists():\n",
        "            shutil.copy2(cand, vdir / cand.name)\n",
        "\n",
        "def main():\n",
        "    _ensure_pkgs()\n",
        "    set_seed_and_fast(CONFIG[\"SEED\"])\n",
        "\n",
        "    src = Path(CONFIG[\"SOURCE_DIR\"])\n",
        "    root= Path(CONFIG[\"DATASET_ROOT\"])\n",
        "    proj= Path(CONFIG[\"PROJECT_DIR\"])\n",
        "    out = Path(CONFIG[\"OUTPUTS_DIR\"])\n",
        "    for p in [root, proj, out]: p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(f\"üîß {gpu_info()} | Seed: {CONFIG['SEED']}\")\n",
        "    print(f\"üì• SOURCE : {src}\")\n",
        "    print(f\"üì¶ ROOT   : {root}\")\n",
        "    print(f\"üìä RUNS   : {proj}\")\n",
        "    print(f\"üóÇÔ∏è OUTPUTS: {out}\")\n",
        "\n",
        "    # 1) ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• + preprocess\n",
        "    create_yolo_dirs(root)\n",
        "    class_map = scan_source(src)\n",
        "    if not class_map or sum(len(v) for v in class_map.values()) == 0:\n",
        "        print(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏π‡∏õ‡πÉ‡∏ô SOURCE_DIR\"); return\n",
        "\n",
        "    classes = sorted(class_map.keys()) if any(src.iterdir()) else CONFIG[\"CLASSES\"]\n",
        "    classes = [c if str(c).strip() else \"object\" for c in classes]\n",
        "\n",
        "    for cname, imgs in class_map.items():\n",
        "        tr, va, te = split_list(imgs, CONFIG[\"SPLIT\"])\n",
        "        for split, lst in [(\"train\",tr),(\"val\",va),(\"test\",te)]:\n",
        "            for img in lst:\n",
        "                base = img.stem\n",
        "                dst_noext = (root/\"images\"/split/base)\n",
        "                k=1\n",
        "                while dst_noext.with_suffix(\".jpg\").exists():\n",
        "                    dst_noext = dst_noext.with_name(f\"{base}_{k}\"); k+=1\n",
        "                final_img = ensure_jpg_resize(img, dst_noext, CONFIG[\"PRE_RESIZE\"], CONFIG[\"SQUARE\"], CONFIG[\"JPEG_QUALITY\"])\n",
        "                dst_lbl  = (root/\"labels\"/split/final_img.stem).with_suffix(\".txt\")\n",
        "                copy_with_label(img, final_img.with_suffix(\"\"), dst_lbl, allow_negative=True)\n",
        "\n",
        "    write_data_yaml(root, classes)\n",
        "    print(f\"üìù ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô data.yaml -> {root/'data.yaml'}\")\n",
        "\n",
        "    # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞ label\n",
        "    for sp in [\"train\",\"val\",\"test\"]:\n",
        "        n = len(list((root/\"images\"/sp).glob(\"*.jpg\")))\n",
        "        print(f\"‚úÖ {sp} images: {n}\")\n",
        "\n",
        "    train_imgs = list((root/\"images\"/\"train\").glob(\"*.jpg\"))\n",
        "    if len(train_imgs) == 0:\n",
        "        print(\"‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏π‡∏õ‡πÉ‡∏ô‡∏ä‡∏∏‡∏î train ‡∏´‡∏•‡∏±‡∏á preprocess (‡πÄ‡∏ä‡πá‡∏Ñ SOURCE_DIR ‡πÅ‡∏•‡∏∞ preprocess)\"); return\n",
        "\n",
        "    val_imgs = list((root/\"images\"/\"val\").glob(\"*.jpg\"))\n",
        "    val_lbl_files = [p for p in (root/\"labels\"/\"val\").glob(\"*.txt\") if p.read_text(encoding=\"utf-8\").strip() != \"\"]\n",
        "    do_val = len(val_imgs) > 0 and len(val_lbl_files) > 0\n",
        "    print(f\"‚ÑπÔ∏è Validation enabled: {do_val}  (val images={len(val_imgs)}, labeled files={len(val_lbl_files)})\")\n",
        "\n",
        "    # 2) Clean outliers\n",
        "    removed_total=0\n",
        "    for sp in [\"train\",\"val\",\"test\"]:\n",
        "        removed_total += remove_outliers(root/\"images\"/sp, CONFIG[\"MIN_SIDE\"], CONFIG[\"ASPECT_Z\"], CONFIG[\"SIZE_Z\"], out/\"clean_log.csv\")\n",
        "    print(f\"üßπ ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏õ‡∏•‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {removed_total}\")\n",
        "\n",
        "    # 2.5) ‡∏Ñ‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏∏‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á\n",
        "    usable=[]\n",
        "    for w in CONFIG[\"VARIANTS\"]:\n",
        "        ok = resolve_variant_name(w)\n",
        "        if ok is None:\n",
        "            print(f\"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏° {w}: ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ\")\n",
        "        else:\n",
        "            if ok != w:\n",
        "                print(f\"‚ÑπÔ∏è ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ '{ok}' ‡πÅ‡∏ó‡∏ô '{w}'\")\n",
        "            usable.append(ok)\n",
        "\n",
        "    if not usable:\n",
        "        print(\"‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏∏‡πà‡∏ô‡πÑ‡∏´‡∏ô‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ (‡∏ï‡∏£‡∏ß‡∏à VARIANTS/‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï)\"); return\n",
        "\n",
        "    # 3) Train per variant\n",
        "    summaries=[]\n",
        "    for wt in usable:\n",
        "        print(\"\\n==============================\")\n",
        "        print(f\"üöÄ ‡πÄ‡∏ó‡∏£‡∏ô‡∏£‡∏∏‡πà‡∏ô: {wt} | do_val={do_val}\")\n",
        "        run_dir, dur = train_one(wt, str(root/\"data.yaml\"), str(proj), wt.replace(\".pt\",\"\"), do_val=do_val)\n",
        "        if run_dir is None:\n",
        "            print(f\"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏ú‡∏• {wt} (‡πÑ‡∏°‡πà‡∏°‡∏µ run_dir)\"); continue\n",
        "        print(f\"‚è±Ô∏è ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ {dur/60:.1f} ‡∏ô‡∏≤‡∏ó‡∏µ | run: {run_dir}\")\n",
        "\n",
        "        # per-epoch graphs\n",
        "        plot_per_epoch(run_dir, out/f\"per_epoch_{run_dir.name}.png\")\n",
        "\n",
        "        # metrics summary\n",
        "        sm = extract_metrics(run_dir)\n",
        "        sm[\"variant\"]=wt\n",
        "        sm[\"train_minutes\"]=f\"{dur/60:.1f}\"\n",
        "        summaries.append(sm)\n",
        "\n",
        "        # üî• ‡∏™‡∏£‡πâ‡∏≤‡∏á Confusion Matrix ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (‡∏ñ‡πâ‡∏≤ val ‡πÑ‡∏°‡πà‡∏°‡∏µ label ‡∏à‡∏∞‡πÉ‡∏ä‡πâ test ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)\n",
        "        eval_split = \"val\" if do_val else \"test\"\n",
        "        weights_path = sm.get(\"weights\", str(run_dir/\"weights\"/\"best.pt\"))\n",
        "        generate_confusion_matrix_png(\n",
        "            weights_path=weights_path,\n",
        "            data_yaml=str(root/\"data.yaml\"),\n",
        "            run_dir=run_dir,\n",
        "            split=eval_split,\n",
        "            iou=0.50,\n",
        "            conf=0.25\n",
        "        )\n",
        "\n",
        "    # 4) Reports & export artifacts\n",
        "    if summaries:\n",
        "        save_summary_csv(summaries, out/\"summary_metrics.csv\")\n",
        "        compare_bar_png(summaries, out/\"compare_metrics.png\")\n",
        "        for s in summaries:\n",
        "            export_variant_artifacts(s, out, Path(s[\"run_dir\"]))\n",
        "        print(\"\\n‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏î‡∏π‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏µ‡πà:\", out)\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è ‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏™‡∏±‡∏Å‡∏£‡∏∏‡πà‡∏ô ‚Äî ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏∏‡πà‡∏ô, dataset, ‡∏´‡∏£‡∏∑‡∏≠ VRAM/‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÑ‡∏ü‡∏•‡πå (‡πÄ‡∏ä‡πà‡∏ô OneDrive)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "trCnwVFL85VF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}